import faiss
import numpy as np
import json
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config import OPENAI_API_KEY, OPENAI_BASE_URL

from langchain.schema import Document  # å¯¼å…¥ Document ç±»

class KnowledgeBase:
    """
    ä»¥ä¸‹ä¸ºä¼ å…¥æ–‡ä»¶æ ¼å¼ä¸ºjsonçš„ç³»ç»Ÿ
    """
    # def __init__(self, data_file="data/documents.json"):
    #     self.data_file = data_file
    #     self.documents = self.load_documents()
    #     self.index = self.create_faiss_index()

    # def load_documents(self):
    #     """ä» JSON æ–‡ä»¶åŠ è½½æ–‡æ¡£"""
    #     with open(self.data_file, "r", encoding="utf-8") as f:
    #         data = json.load(f)
    #         return data.get("documents", [])  # ç¡®ä¿è¿”å›åˆ—è¡¨ï¼Œé¿å… NoneType é”™è¯¯

    # def create_faiss_index(self):
    #     """åˆ›å»º FAISS å‘é‡ç´¢å¼•"""
    #     embeddings = DashScopeEmbeddings(
    #         model="text-embedding-v2",
    #         dashscope_api_key=OPENAI_API_KEY,
    #         # dashscope_api_base=OPENAI_BASE_URL
    #     )

    #     texts = [doc.get("content", "").strip() for doc in self.documents if isinstance(doc.get("content"), str)]

    #     # ğŸ”´ ç¡®ä¿ texts éç©º
    #     if not texts:
    #         raise ValueError("âŒ æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºåµŒå…¥å‘é‡")

    #     try:
    #         # æ‹†åˆ†æ–‡æœ¬
    #         text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
    #         docs = text_splitter.create_documents(texts)  # è¿™é‡Œè¿”å›çš„æ˜¯ Document ç±»å‹çš„åˆ—è¡¨

    #         # å‘é€ç»™ OpenAI
    #         texts = [doc.page_content for doc in docs]

                
    #         vector_store = FAISS.from_texts(texts, embeddings)  

    #     except Exception as e:
    #         print("âŒ OpenAI å¤„ç†å¤±è´¥:", e)
    #         raise e  # æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢ç¨‹åº

    #     return vector_store

    # def search(self, query, top_k=3):
    #     """åœ¨ FAISS ç´¢å¼•ä¸­æŸ¥æ‰¾æœ€ç›¸å…³çš„æ–‡æ¡£"""
    #     results = self.index.similarity_search(query, k=top_k)
    #     return [r.page_content for r in results]
    """
    ä»¥ä¸‹ä¸ºä¼ å…¥æ–‡ä»¶æ ¼å¼ä¸ºtxtçš„ç³»ç»Ÿ
    """
    def __init__(self, data_file="data/0710.txt"):
        self.data_file = data_file
        self.documents = self.load_documents()
        self.index = self.create_faiss_index()

    def load_documents(self):
        """ä» TXT æ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        documents = []
        try:
            with open(self.data_file, "r", encoding="utf-8") as f:
                lines = f.readlines()  # è¯»å–æ‰€æœ‰è¡Œ
                for line in lines:
                    # æ¯ä¸€è¡Œä½œä¸ºä¸€ä¸ªæ–‡æ¡£ï¼Œå¹¶ä½œä¸ºå­—å…¸æ ¼å¼å­˜å‚¨
                    documents.append({"content": line.strip()})
        except Exception as e:
            print("âŒ åŠ è½½æ–‡æ¡£å¤±è´¥:", e)
        return documents

    def create_faiss_index(self):
        """åˆ›å»º FAISS å‘é‡ç´¢å¼•"""
        embeddings = DashScopeEmbeddings(
            model="text-embedding-v2",
            dashscope_api_key=OPENAI_API_KEY,
            # dashscope_api_base=OPENAI_BASE_URL
        )

        texts = [doc.get("content", "").strip() for doc in self.documents if isinstance(doc.get("content"), str)]

        # ğŸ”´ ç¡®ä¿ texts éç©º
        if not texts:
            raise ValueError("âŒ æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºåµŒå…¥å‘é‡")

        try:
            # æ‹†åˆ†æ–‡æœ¬
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
            docs = text_splitter.create_documents(texts)  # è¿™é‡Œè¿”å›çš„æ˜¯ Document ç±»å‹çš„åˆ—è¡¨

            # å‘é€ç»™ OpenAI
            texts = [doc.page_content for doc in docs]

                
            vector_store = FAISS.from_texts(texts, embeddings)  

        except Exception as e:
            print("âŒ OpenAI å¤„ç†å¤±è´¥:", e)
            raise e  # æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢ç¨‹åº

        return vector_store

    def search(self, query, top_k=3):
        """åœ¨ FAISS ç´¢å¼•ä¸­æŸ¥æ‰¾æœ€ç›¸å…³çš„æ–‡æ¡£"""
        results = self.index.similarity_search(query, k=top_k)
        return [r.page_content for r in results]

kb = KnowledgeBase()

